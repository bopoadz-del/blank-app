# Traditional Machine Learning Framework

Comprehensive implementation of traditional machine learning algorithms using Scikit-learn, XGBoost, and LightGBM.

## ğŸ“‹ Features

### Classification Algorithms
- **Random Forest**: Ensemble of decision trees with bagging
- **XGBoost**: Gradient boosting with advanced regularization
- **LightGBM**: Fast gradient boosting with histogram-based learning
- **SVM**: Support Vector Machines with various kernels
- **KNN**: K-Nearest Neighbors with distance metrics
- **Decision Trees**: CART algorithm with pruning
- **Naive Bayes**: Gaussian, Multinomial, and Bernoulli variants
- **Logistic Regression**: Linear classification with regularization

### Regression Algorithms
- **Linear Regression**: Ordinary Least Squares with regularization
- **Ridge Regression**: L2 regularization
- **Lasso Regression**: L1 regularization
- **ElasticNet**: Combined L1 and L2 regularization
- **Random Forest Regressor**: Ensemble regression
- **XGBoost Regressor**: Gradient boosting for regression
- **LightGBM Regressor**: Fast gradient boosting
- **SVR**: Support Vector Regression

### Clustering Algorithms
- **K-Means**: Centroid-based clustering
- **DBSCAN**: Density-based clustering
- **Hierarchical Clustering**: Agglomerative clustering
- **Gaussian Mixture Models**: Probabilistic clustering
- **MeanShift**: Mode-seeking clustering
- **Spectral Clustering**: Graph-based clustering

### Dimensionality Reduction
- **PCA**: Principal Component Analysis
- **t-SNE**: t-Distributed Stochastic Neighbor Embedding
- **UMAP**: Uniform Manifold Approximation and Projection
- **LDA**: Linear Discriminant Analysis
- **Truncated SVD**: Singular Value Decomposition
- **Kernel PCA**: Non-linear PCA

### Feature Selection
- **Univariate Selection**: Statistical tests (chi2, f_classif, mutual_info)
- **Recursive Feature Elimination**: RFE with cross-validation
- **Feature Importance**: Tree-based importance
- **L1-based Selection**: Lasso feature selection
- **Variance Threshold**: Remove low-variance features
- **Correlation Analysis**: Remove highly correlated features

### Model Utilities
- **Cross-Validation**: K-fold, stratified, time-series
- **Hyperparameter Tuning**: Grid search, random search, Bayesian optimization
- **Model Evaluation**: Comprehensive metrics for classification and regression
- **Pipeline Creation**: End-to-end ML pipelines
- **Model Persistence**: Save and load models

## ğŸš€ Quick Start

### Classification Example

```python
from classifiers.ensemble import RandomForestClassifier
from utils.evaluation import ModelEvaluator
from sklearn.datasets import make_classification

# Create dataset
X, y = make_classification(n_samples=1000, n_features=20)

# Train model
clf = RandomForestClassifier(n_estimators=100, max_depth=10)
clf.fit(X_train, y_train)

# Evaluate
evaluator = ModelEvaluator(clf, X_test, y_test)
metrics = evaluator.evaluate_classification()
print(f"Accuracy: {metrics['accuracy']:.4f}")
```

### Regression Example

```python
from regressors.ensemble import XGBoostRegressor
from utils.evaluation import ModelEvaluator

# Train model
reg = XGBoostRegressor(n_estimators=100, learning_rate=0.1)
reg.fit(X_train, y_train)

# Evaluate
evaluator = ModelEvaluator(reg, X_test, y_test)
metrics = evaluator.evaluate_regression()
print(f"RÂ² Score: {metrics['r2_score']:.4f}")
```

### Clustering Example

```python
from clustering.density import DBSCANClustering
from utils.visualization import plot_clusters

# Cluster data
clusterer = DBSCANClustering(eps=0.5, min_samples=5)
labels = clusterer.fit_predict(X)

# Visualize
plot_clusters(X, labels)
```

### Dimensionality Reduction Example

```python
from dimensionality_reduction.linear import PCAReducer
from dimensionality_reduction.nonlinear import TSNEReducer

# PCA
pca = PCAReducer(n_components=2)
X_pca = pca.fit_transform(X)

# t-SNE
tsne = TSNEReducer(n_components=2, perplexity=30)
X_tsne = tsne.fit_transform(X)
```

### Feature Selection Example

```python
from feature_selection.statistical import UnivariateSelector
from feature_selection.model_based import TreeBasedSelector

# Univariate selection
selector = UnivariateSelector(score_func='f_classif', k=10)
X_selected = selector.fit_transform(X, y)

# Tree-based selection
selector = TreeBasedSelector(estimator='random_forest', threshold='median')
X_selected = selector.fit_transform(X, y)
```

### Hyperparameter Tuning Example

```python
from utils.tuning import HyperparameterTuner
from classifiers.ensemble import RandomForestClassifier

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10]
}

# Tune hyperparameters
tuner = HyperparameterTuner(
    RandomForestClassifier(),
    param_grid,
    method='grid_search',
    cv=5
)

best_model = tuner.fit(X_train, y_train)
print(f"Best parameters: {tuner.best_params}")
```

## ğŸ“ Project Structure

```
traditional-ml/
â”œâ”€â”€ classifiers/
â”‚   â”œâ”€â”€ ensemble.py          # Random Forest, XGBoost, LightGBM
â”‚   â”œâ”€â”€ linear.py            # Logistic Regression
â”‚   â”œâ”€â”€ svm.py               # Support Vector Machines
â”‚   â”œâ”€â”€ neighbors.py         # K-Nearest Neighbors
â”‚   â”œâ”€â”€ tree.py              # Decision Trees
â”‚   â””â”€â”€ naive_bayes.py       # Naive Bayes variants
â”œâ”€â”€ regressors/
â”‚   â”œâ”€â”€ linear.py            # Linear, Ridge, Lasso, ElasticNet
â”‚   â”œâ”€â”€ ensemble.py          # RF, XGBoost, LightGBM regressors
â”‚   â””â”€â”€ svm.py               # Support Vector Regression
â”œâ”€â”€ clustering/
â”‚   â”œâ”€â”€ centroid.py          # K-Means
â”‚   â”œâ”€â”€ density.py           # DBSCAN
â”‚   â”œâ”€â”€ hierarchical.py      # Agglomerative clustering
â”‚   â””â”€â”€ probabilistic.py     # Gaussian Mixture Models
â”œâ”€â”€ dimensionality_reduction/
â”‚   â”œâ”€â”€ linear.py            # PCA, LDA
â”‚   â”œâ”€â”€ nonlinear.py         # t-SNE, UMAP
â”‚   â””â”€â”€ manifold.py          # Other manifold methods
â”œâ”€â”€ feature_selection/
â”‚   â”œâ”€â”€ statistical.py       # Univariate selection
â”‚   â”œâ”€â”€ model_based.py       # RFE, feature importance
â”‚   â””â”€â”€ variance.py          # Variance threshold
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ evaluation.py        # Model evaluation metrics
â”‚   â”œâ”€â”€ tuning.py            # Hyperparameter tuning
â”‚   â”œâ”€â”€ pipeline.py          # ML pipelines
â”‚   â”œâ”€â”€ preprocessing.py     # Data preprocessing
â”‚   â””â”€â”€ visualization.py     # Plotting utilities
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ classification_example.py
â”‚   â”œâ”€â”€ regression_example.py
â”‚   â”œâ”€â”€ clustering_example.py
â”‚   â””â”€â”€ feature_selection_example.py
â””â”€â”€ README.md
```

## ğŸ“Š Model Comparison

### Classification Performance
| Model | Accuracy | Training Time | Interpretability |
|-------|----------|---------------|------------------|
| Random Forest | â­â­â­â­ | â­â­â­ | â­â­â­ |
| XGBoost | â­â­â­â­â­ | â­â­ | â­â­ |
| LightGBM | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| SVM | â­â­â­â­ | â­â­ | â­â­ |
| KNN | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| Decision Tree | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| Naive Bayes | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| Logistic Reg | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

## ğŸ”§ Installation

```bash
pip install scikit-learn xgboost lightgbm umap-learn
pip install numpy pandas matplotlib seaborn
```

## ğŸ“š Documentation

Each module contains detailed documentation with:
- Algorithm description
- Parameter explanations
- Usage examples
- Best practices
- Performance considerations

## ğŸ¯ Best Practices

1. **Always split your data**: Use train-test split or cross-validation
2. **Scale your features**: Most algorithms benefit from feature scaling
3. **Handle missing values**: Use imputation or remove missing data
4. **Tune hyperparameters**: Use grid search or random search
5. **Evaluate properly**: Use appropriate metrics for your problem
6. **Check for overfitting**: Monitor train vs. validation performance
7. **Use pipelines**: Combine preprocessing and modeling steps

## ğŸš€ Performance Tips

### Random Forest
- Increase `n_estimators` for better performance
- Use `max_features='sqrt'` for classification
- Set `n_jobs=-1` for parallel processing

### XGBoost
- Start with `learning_rate=0.1` and `n_estimators=100`
- Use early stopping to prevent overfitting
- Tune `max_depth` and `min_child_weight`

### LightGBM
- Use `num_leaves` instead of `max_depth`
- Enable `categorical_feature` for categorical data
- Set `bagging_fraction < 1` to prevent overfitting

### SVM
- Scale features before training
- Use RBF kernel for non-linear problems
- Tune `C` and `gamma` with grid search

## ğŸ“ˆ Metrics Reference

### Classification Metrics
- **Accuracy**: Overall correctness
- **Precision**: Positive predictive value
- **Recall**: Sensitivity, true positive rate
- **F1-Score**: Harmonic mean of precision and recall
- **ROC-AUC**: Area under ROC curve
- **Confusion Matrix**: True/false positives/negatives

### Regression Metrics
- **RÂ² Score**: Coefficient of determination
- **MSE**: Mean Squared Error
- **RMSE**: Root Mean Squared Error
- **MAE**: Mean Absolute Error
- **MAPE**: Mean Absolute Percentage Error

### Clustering Metrics
- **Silhouette Score**: Cluster cohesion
- **Calinski-Harabasz Index**: Cluster separation
- **Davies-Bouldin Index**: Cluster compactness

## ğŸ“ License

This project is provided as-is for educational and commercial use.
